<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[TTimeline]]></title>
  <subtitle><![CDATA[星辰大海]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://tongsucn.com/"/>
  <updated>2015-11-05T21:53:08.000Z</updated>
  <id>http://tongsucn.com/</id>
  
  <author>
    <name><![CDATA[Tong Su]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[[ML Review] Least Squares Regression]]></title>
    <link href="http://tongsucn.com/2015/11/05/least-squares-regression/"/>
    <id>http://tongsucn.com/2015/11/05/least-squares-regression/</id>
    <published>2015-11-05T21:32:20.000Z</published>
    <updated>2015-11-05T21:53:08.000Z</updated>
    <content type="html"><![CDATA[<script tpye"text="" x-mathjax-config"="">
MathJax.Hub.Config({tex2jax: {inlinMath: [['$', '$'], ['\\(', '\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>In machine learning, regression is utilized to predict continuous values. Least squares regression is one of the simplest regression technology. Given the training set: \( \text{X} = \{x_1, …, x_N\} \) with target values \( T = \{t_1, …, t_N\} \). Our goal is to obtain a function, which can “best” describe these points and predict other input points’ target values. But how can we obtain such a function?</p>
<p>Firstly, we need to “guess” the form of this function. The simplest is the linear function: \( y = kx + b \), just like what we have learnt in middle school. Let’s write it as \( t = w \cdot x + w_0 \), where \(t\) is the target value, \(w\) is the weight and \(w_0\) is bias. Therefore, to get this function, our goal now becomes calculating the value of \(w\) and \(w_0\).</p>
<p>It is obvious that we need to use some points to calculate the weight and bias. Such points named training sets. In general, training sets contain two parts: some data points \(\text{X} = [x_1, …, x_N] \) and their labels (or target values) \( t = [t_1, …, t_N] \). The formula becomes \( w^T \text{X} + w_0 = t \). In most cases we can write it as \( \widetilde{\text{X}}^T \widetilde{\text{w}}^T = t \), where \( \widetilde{\text{w}} = [w, w_0]^T \), \( \widetilde{\text{X}} = [\widetilde{x}_1, …, \widetilde{x}_N] \) and \( {\widetilde{x}}_i = [x_i, 1]^T \).</p>
<p>How to calculate \( \widetilde{\text{w}} \) now? Let’s consider about it from another perspective. If a suitable \( \widetilde{\text{w}} \) is given, we can easily calculate a set of labels \( t^\prime \) for the above training set \( \widetilde{\text{X}} \). But in general the components in \( t - t^\prime \) cannot be zero because the points don’t fit perfectly. (See the picture below.)</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_1.png" alt="cannot-fit-perfectly" title="Cannot fit perfectly"></p>
<p>In other words, if there exists a \( \widetilde{\text{w}} \), which makes the difference between \( t^\prime \) and \( t \) minimal, then the this linear function with \( \widetilde{\text{w}} \) can “best” describe these points. Such \( \widetilde{\text{w}} \) is what we want. Therefore we can get it via computing the derivative of \( E(\widetilde{\text{w}}) = \left|\left| \widetilde{\text{X}}^T \widetilde{\text{w}}- t \right|\right|^2  \):</p>
<p>$$<br>\begin{align} &amp;&amp; \frac{\partial E(\widetilde{\text{w}})}{\partial \widetilde{\text{w}}} &amp; = 2 \widetilde{\text{X}} (\widetilde{\text{X}}^T \widetilde{\text{w}} - t) \stackrel{!}{=} 0 \\ &amp; \Rightarrow &amp; \widetilde{\text{X}} \widetilde{\text{X}}^T \widetilde{\text{w}} &amp; = \widetilde{\text{X}} t \\ &amp; \Rightarrow &amp; \quad \widetilde{\text{w}} &amp; = (\widetilde{\text{X}} \widetilde{\text{X}}^T)^{-1} \widetilde{\text{X}} t \end{align}<br>$$</p>
<p>The <a href="https://github.com/tongsucn/MachineLearningPractice/tree/master/Regression" target="_blank" rel="external">Python code</a>. (Python 3 + numpy)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">training_data_point = ...</span><br><span class="line">training_data_label = ...</span><br><span class="line">training_data_num = ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lsr_train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Training least squares regression.</span><br><span class="line">    Returns:</span><br><span class="line">        The weights w.</span><br><span class="line">    """</span></span><br><span class="line">    <span class="comment"># Extending data points with an additional dimensional of value one</span></span><br><span class="line">    ext_points = ext_data(training_data_point, training_data_num)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculating and returning w</span></span><br><span class="line">    res_w = np.dot(ext_points, np.transpose(ext_points))</span><br><span class="line">    res_w = np.dot(np.linalg.inv(res_w), ext_points)</span><br><span class="line">    <span class="keyword">return</span> np.dot(res_w, np.transpose(training_data_label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ext_data</span><span class="params">(target_data, length, axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Extending data points with an additional dimension of value one.</span><br><span class="line">    Args:</span><br><span class="line">        target_data: The 1-D data points to be extended.</span><br><span class="line">        length: The number of input data points.</span><br><span class="line">        axis: The dimension to be extended, 0 by default.</span><br><span class="line">    Returns:</span><br><span class="line">        The extended data points.</span><br><span class="line">    """</span></span><br><span class="line">    one = np.ones((length, <span class="number">1</span>)) <span class="keyword">if</span> axis <span class="keyword">else</span> np.ones((<span class="number">1</span>, length))</span><br><span class="line">    <span class="keyword">return</span> np.concatenate((target_data, one), axis)</span><br></pre></td></tr></table></figure>
<p>Let’s see its result on a given training set:</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_2.png" alt="training-on-non-linear-data" title="Linear function on non-linear data set"></p>
<p>Although the obtained line describes the trend of the data points, but it’s not what we want. Why? Because it doesn’t fit the points well, the data points are not linear!</p>
<p>To be continued…</p>
]]></content>
    <summary type="html">
    <![CDATA[<script tpye"text="" x-mathjax-config"="">
MathJax.Hub.Config({tex2jax: {inlinMath: [['$', '$'], ['\\(', '\\)']]}});
</script>
<script type=]]>
    </summary>
    
      <category term="algorithms" scheme="http://tongsucn.com/tags/algorithms/"/>
    
      <category term="basis function" scheme="http://tongsucn.com/tags/basis-function/"/>
    
      <category term="least squares regression" scheme="http://tongsucn.com/tags/least-squares-regression/"/>
    
      <category term="machine learning" scheme="http://tongsucn.com/tags/machine-learning/"/>
    
      <category term="mean squared error" scheme="http://tongsucn.com/tags/mean-squared-error/"/>
    
      <category term="regression" scheme="http://tongsucn.com/tags/regression/"/>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[First Post]]></title>
    <link href="http://tongsucn.com/2015/03/06/First-Post/"/>
    <id>http://tongsucn.com/2015/03/06/First-Post/</id>
    <published>2015-03-06T08:30:20.000Z</published>
    <updated>2015-10-30T10:45:13.000Z</updated>
    <content type="html"><![CDATA[<p>Hello world!</p>
<p>This is my new blog. I will begin to record my life and study here. My interest are Computer Vision and Machine Learning.</p>
<p>It will be greatly honoured if my words here could help you.</p>
<p>Tong Su</p>
<p>MathJax Test:</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>$$E=mc^2$$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hello world!</p>
<p>This is my new blog. I will begin to record my life and study here. My interest are Computer Vision and Machine Learn]]>
    </summary>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
</feed>