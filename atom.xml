<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[TTimeline]]></title>
  <subtitle><![CDATA[星辰大海]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://tongsucn.com/"/>
  <updated>2015-11-24T09:32:26.000Z</updated>
  <id>http://tongsucn.com/</id>
  
  <author>
    <name><![CDATA[Tong Su]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[[ML Review] Regression from Probabilistic Perspective]]></title>
    <link href="http://tongsucn.com/2015/11/24/ML-Review-Regression-from-Probability-Perspective/"/>
    <id>http://tongsucn.com/2015/11/24/ML-Review-Regression-from-Probability-Perspective/</id>
    <published>2015-11-24T09:22:20.000Z</published>
    <updated>2015-11-24T09:32:26.000Z</updated>
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(', '\\)']]}});
</script>
<script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

<h2 id="Introduction">Introduction</h2><p>I have reviewed the <a href="/2015/11/05/ML-Review-Least-Squares-Regression/">Least Squares Regression</a> (LSR) and <a href="/2015/11/11/ML-Review-Ridge-Regression-and-Regularization/">Ridge Regression</a> (RR). In this post, I will review them again, but from probabilistic perspective.</p>
<h2 id="Probabilistic_Interpretation:_LSR">Probabilistic Interpretation: LSR</h2><p>In LSR and RR, we optimized the results by minimizing the error (sum of squares error was utilized). We can find the training error cannot be 0, it’s not quite precise. This is because in general, the data sets contain noise. Therefore, if we want to consider the LSR and RR from probability perspective, we could use probability to express those noise. For example, given a training set, let’s assume the noise of points in this set have the Gaussian distribution like this (The Gaussian distribution curve has been adjusted for better appearance):</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]RPP_1.png" width="256"></p>
<p>Our target function values \( t \) is now:</p>
<p>$$<br>t = \widetilde{\text{w}}^T \widetilde{\text{x}} + \epsilon = y(\text{x}, \text{w}) + \epsilon<br>$$</p>
<p>\( \epsilon \) here is the noise, which has Gaussian distribution, \( y(\text{x}, \text{w}) \) is the target curve (or ideally we can say it’s the curve without error). Transforming it into probability form, for every single point \( \text{x} \) it will look like:</p>
<p>$$<br>p(t | \text{x}, \text{w}, \beta) = \mathcal{N} (t | y(\text{x}, \text{w}), \beta^{-1})<br>$$</p>
<p>It means that given a point \( \text{x} \), relative weighting vector \( \text{w} \) and the precision parameter \( \beta \) (talk about it later), the target value \( t \) has Gaussian distribution. Therefore, to learn the weighting vector \( \text{w}\) and \( \beta \), we can just maximize the conditional likelihood with the training set \( \text{X} \) and its label set \( \text{t}\):</p>
<p>$$<br>p(\text{t} | \text{X}, \text{w}, \beta) = \prod_{n = 1}^{N} {\mathcal{N} (t_n | y(\text{x}_n, \text{w}), \beta^{-1})}<br>$$</p>
<p>For simplifying the calculation, writing it in \( \log \) form:</p>
<p>$$<br>\begin{align} \log{p(\text{t} | \text{X}, \text{w}, \beta)} &amp; = \sum_{n = 1}^{N} {\log{\mathcal{N} (t_n | y(\text{x}_n, \text{w}), \beta^{-1})}} \\ &amp; = \sum_{n = 1}^{N} { \left[\log{( \sqrt{\frac{\beta} {2 \pi}})} - \frac{\beta} {2} \{ y(\text{w}_n, \text{w}) - t_n \}^2 \right] } \\ &amp; = \frac{N} {2} \log{\beta} - \frac{N} {2} \log{(2 \pi)} - \frac{\beta} {2} \sum_{n = 1}^{N} \{ t_n - y(\text{x}_n, \text{w}) \} \end{align}<br>$$</p>
<p>where \( \frac{N} {2} \log{\beta} - \frac{N} {2} \log{(2 \pi)} \) is a constant, and the rest is the sum of squares error. Calculating its Gradient w.r.t \( \text{w} \) we can get:</p>
<p>$$<br>\begin{align} \nabla_{\text{w}} \log {p(\text{t} | \text{X}, \text{w}, \beta)} &amp; = - \beta \sum_{n = 1}^{N} {(t_n - \text{w}^T \phi(\text{x}_n)) \phi(\text{x}_n)} \stackrel{!}{=} 0 \\ &amp; \Rightarrow \sum_{n - 1}^{N} {t_n \phi(\text{x}_n)} = [\sum_{n - 1}^{N} {\phi(\text{x}_n) \phi(\text{x}_n)^T}] \text{w} \\ &amp; \Rightarrow \phi(\text{X})^T \text{t} = \phi(\text{X}) \phi(\text{X})^T \text{w} \\ &amp; \Rightarrow \hat{\text{w}} = (\phi(\text{X}) \phi(\text{X})^T)^{-1} \phi(\text{X}) \text{t} \end{align}<br>$$</p>
<p>Does it look familiar? It’s the same with LSR’s optimization result! We should have already got a better understanding about what Linear Squares Regression is. The LSR is equivalent to <a href="https://en.wikipedia.org/wiki/Maximum_likelihood" target="_blank" rel="external">Maximum Likelihood Estimation</a> under the assumption of Gaussian noise. Now let’s take a look at the previous mentioned \( \beta \). If we calculate the gradient w.r.t \( \beta \), we can get the optimization result like this (do it yourself :D):</p>
<p>$$<br>\frac{1} {\hat{\beta}} = \frac{1} {N} \sum_{n = 1}^{N} {[t_n - \hat{\text{w}}^T \phi(\text{x}_{n}) ]^2}<br>$$</p>
<p>The higher precision the \( \beta \) has, the lower mean squared error the data there would be. After getting these two values, we can predict new input with the following distribution:</p>
<p>$$<br>p(t | \text{x}, \hat{\text{w}}, \hat{\beta}) = \mathcal{N} \left( t | y(\text{x}, \hat{\text{w}}), \hat{\beta}^{-1} \right)<br>$$</p>
<p>So far, these illustrations are still based on the <a href="https://en.wikipedia.org/wiki/Frequentist_inference" target="_blank" rel="external">frequentist approach</a>, let’s now stride towards the Bayesian approaches.</p>
<h2 id="Probabilistic_Interpretation:_RR">Probabilistic Interpretation: RR</h2><p>Because of the equivalence between LSR and Maximum Likelihood Estimation under assumption of Gaussian noise, they also share the same challenge: overfitting. I have illustrated it in <a href="/2015/11/05/ML-Review-Least-Squares-Regression/">this post</a>. To avoid overfitting, in Ridge Regression, the regularization coefficient \( \lambda \) is involved to penalize the larger size weighting vectors. One way to get such \( \lambda \) is to take advantage of the empirical knowledge. So what is similar to “empirical knowledge” in probability field? The prior! Even so, it sounds quite abstract, let me illustrate it in formula.</p>
<p>Firstly I want to repeat our previous question again: Given a training set \( \text{X} \) and relative labels \( \text{t} \), our goal is to learn a single weighting vector \( \text{w} \), which “best” describes the data points. To avoid overfitting, we can provide some “guidance” to our model. Such “guidance” is a prior, which describes the prior distribution of the weighting vector \( \text{w} \). <strong>ATTENTION</strong>: it is the distribution of \( \text{w} \), rather than a single value of it.</p>
<p>Since it is a distribution, there must exist some parameters describing this distribution. We call such parameters the <a href="https://en.wikipedia.org/wiki/Hyperparameter" target="_blank" rel="external">hyperparameter</a>. For example, assume a given prior describing the \( \text{w} \) in Gaussian distribution. The hyperparameter \( \alpha = \left( m_{0}, S_{0} \right)\), where \( m_{0} \) is the mean, \( S_{0} \) is the variance. Therefore the distribution shall look like this:</p>
<p>$$<br>p(\text{w} | \alpha) = \mathcal{N} \left(\text{w} | m_{0}, S_{0} \text{I} \right) = \left( \frac{1} {2 \pi S_{0}} \right)^{(M + 1) / 2} \exp \left(- \frac{1} {2 S_{0}} (\text{w} - m_{0})^T (\text{w} - m_{0}) \right)<br>$$</p>
<p>Our posterior <strong>distribution</strong> over \( \text{w} \) is extended and according to the <a href="https://goo.gl/3K9Ymv" target="_blank" rel="external">Bayes’ Theorem</a>:</p>
<p>$$<br>p(\text{w} | \text{X}, \text{t}, \beta, \alpha) \propto p(\text{t} | \text{X}, \text{w}, \beta) p(\text{w} | \alpha)<br>$$</p>
<p>Minimizing its negative \( \log \) form:</p>
<p>$$<br>\begin{align} - \log {p(\text{w} | \text{X}, \text{t}, \beta, \alpha)} &amp; \propto - \log{p(\text{t} | \text{X}, \text{w}, \beta)} - \log{p(\text{w} | \alpha)} \\ &amp; \propto - \log{ \prod_{n = 1}^{N} {\mathcal{N} (t_n | y(\text{x}_n, \text{w}), \beta^{-1})} } - \log{ \exp \left(- \frac{1} {2 S_{0}} (\text{w} - m_{0})^T (\text{w} - m_{0}) \right) }  \\ &amp; \propto \frac{\beta}{2} \sum_{n = 1}^{N} \{ y(\text{x}_n, \text{w}) - t_n\}^2 + \frac{1} {2 S_0} (\text{w} - m_{0})^T (\text{w} - m_{0}) + \text{const} \\ &amp; \propto \frac{\beta}{2} \left( \text{w}^T \phi(\text{X}) - \text{t} \right)^2 + \frac{1} {2 S_0} (\text{w} - m_{0})^T (\text{w} - m_{0}) + \text{const} \end{align}<br>$$</p>
<p>Calculating the gradient we can get the optimized weighting vector \( \hat{\text{w}} \):</p>
<p>$$<br>\begin{align} \nabla_{\text{w}} \log {p(\text{t} | \text{X}, \text{w}, \beta, \alpha)} &amp; = - \beta \sum_{n = 1}^{N} {(t_n - \text{w}^T \phi(\text{x}_n)) \phi(\text{x}_n)} + {S_{0}}^{-1} (\text{w} - m_{0}) \stackrel{!}{=} 0 \\ &amp; \Rightarrow \beta \cdot \phi(\text{X})^T \text{t}  + {S_{0}}^{-1} m_{0} = \left( \beta \cdot \phi(\text{X}) \phi(\text{X})^T + {S_{0}}^{-1} \text{I} \right) \text{w} \\ &amp; \Rightarrow \hat{\text{w}} = \left( \beta \cdot \phi(\text{X}) \phi(\text{X})^T + {S_{0}}^{-1} \text{I} \right)^{-1} \left( \beta \cdot \phi(\text{X})^T \text{t}  + {S_{0}}^{-1} m_{0} \right) \end{align}<br>$$</p>
<p>The result looks quite complicated, but in general, for simplification, we often set the prior mean \( m_{0} = 0\). Then it will similar to the RR optimization result, and the regularization parameter \( \lambda \) in Ridge Regression will be equal to \( \frac{1} {S_{0} \beta} \). This is the so called <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" target="_blank" rel="external">Maximum a posteriori (MAP) estimation</a>.</p>
<h2 id="Summary">Summary</h2><p>So far, we have a better understanding to both of the ordinary linear regression (Least Squares Regression) and the regularized regression (Ridge Regression). The Least Squares Regression is equivalent to Maximum Likelihood Estimation under Gaussian noise assumption, and they share the same problem: overfitting. The Ridge Regression is equivalent to Maximum a Posterior estimation with a Gaussian prior distribution over weighting vector \( \text{w} \). The probabilistic interpretation to RR is already alike to the Bayesian estimation. In next post, I will try to introduce the Bayesian Curve Fitting. Hope it could help :D</p>
<h2 id="Reference">Reference</h2><p>[1] <a href="/2015/11/05/ML-Review-Least-Squares-Regression/">Least Squares Regression</a><br>[2] <a href="/2015/11/11/ML-Review-Ridge-Regression-and-Regularization/">Ridge Regression</a><br>[3] <a href="https://en.wikipedia.org/wiki/Maximum_likelihood" target="_blank" rel="external">Maximum Likelihood Estimation</a><br>[4] <a href="https://en.wikipedia.org/wiki/Frequentist_inference" target="_blank" rel="external">frequentist approach</a><br>[5] <a href="https://en.wikipedia.org/wiki/Hyperparameter" target="_blank" rel="external">hyperparameter</a><br>[6] <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" target="_blank" rel="external">Maximum a posteriori (MAP) estimation</a><br>[7] <a href="https://goo.gl/3K9Ymv" target="_blank" rel="external">Bayes’ Theorem</a><br>[8] Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.</p>
]]></content>
    <summary type="html">
    <![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(', '\\)']]}});
</script>
<script type="text/javascript]]>
    </summary>
    
      <category term="Bayesian" scheme="http://tongsucn.com/tags/Bayesian/"/>
    
      <category term="Gaussian distribution" scheme="http://tongsucn.com/tags/Gaussian-distribution/"/>
    
      <category term="least squares regression" scheme="http://tongsucn.com/tags/least-squares-regression/"/>
    
      <category term="likelihood" scheme="http://tongsucn.com/tags/likelihood/"/>
    
      <category term="machine learning" scheme="http://tongsucn.com/tags/machine-learning/"/>
    
      <category term="maximum a posteriori" scheme="http://tongsucn.com/tags/maximum-a-posteriori/"/>
    
      <category term="maximum likelihood estimation" scheme="http://tongsucn.com/tags/maximum-likelihood-estimation/"/>
    
      <category term="posterior" scheme="http://tongsucn.com/tags/posterior/"/>
    
      <category term="prior" scheme="http://tongsucn.com/tags/prior/"/>
    
      <category term="probability interpretation" scheme="http://tongsucn.com/tags/probability-interpretation/"/>
    
      <category term="regression" scheme="http://tongsucn.com/tags/regression/"/>
    
      <category term="ridge regression" scheme="http://tongsucn.com/tags/ridge-regression/"/>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[ML Review] Ridge Regression & Regularization]]></title>
    <link href="http://tongsucn.com/2015/11/11/ML-Review-Ridge-Regression-and-Regularization/"/>
    <id>http://tongsucn.com/2015/11/11/ML-Review-Ridge-Regression-and-Regularization/</id>
    <published>2015-11-11T10:18:52.000Z</published>
    <updated>2015-11-14T14:21:12.000Z</updated>
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(', '\\)']]}});
</script>
<script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

<h2 id="Introduction">Introduction</h2><p>In the <a href="/2015/11/05/ML-Review-Least-Squares-Regression">previous post</a>, the Least Squares Regression (LSR) is reviewed. But the last shown example overfits when the frequency becomes higher (the basis function maps input points into higher dimensional feature space). This post will review a method called Ridge Regression against such overfitting. Furthermore, the concept of regularization will also be introduced.</p>
<h2 id="Ridge_Regression">Ridge Regression</h2><p>Before formally introduction to the Ridge Regression, let’s firstly think about, why overfitting happens in LSR. In our previous example, when frequency is \( 1 \), the trained function is not “flexible” enough. It can only express one pair of “wave crest” and “wave trough” within our training data’s range. Because of such limitation, the optimization have to compromise. Calculate a minimum error and getting a “trend description” as the final result.</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_3.png" width="256"></p>
<p>When the frequency increases to a larger number, the result functions become quite flexible. There are enough local “crest” and “trough” to fit the training points, which lead to local “trembling”, or in other words, overfitting. Our goal is now clear, we need to suppress such local “trembling” and keep its generally trend.</p>
<p>In our error function: \( E(\text{w}) = \left|\left| t - \text{w}^T \text{X} \right|\right|^2 \), with the increasement of basis function mapped feature space’s dimensional, the only changed thing is the weighting vector \( \text{w} \)’s dimension. If we penalize \( \text{w} \)’s size (number of dimension), results would be better: \( E_{ridge} (\text{w}) = \left|\left| t - \text{w}^T \text{X} \right|\right|^2 + \lambda \left|\left| \text{w} \right|\right|^2 \). Given a suitable \( \lambda \) (scalar, talk about its value selection later), higher \( \text{w} \) size will be penalized and “trembling” will be suppressed.</p>
<p>Not quite intuitive? Assume that \( \text{w}_{opt} \) is the optimized result from LSR error function \( E(\text{w}) \). But it could not be the minimum-value-point in Ridge Regression error function \( E_{ridge} (\text{w}) \) because of the insertion of \( \lambda \left|\left| \text{w}_{opt} \right|\right|^2 \). Therefore to achieve the minimum error value, the optimization procedure will adjust each component value in weighting vector \( \text{w} \). Let’s look at the comparison of variance (the table below). After observation we can find that the variance of components in each weighting vector decreased, which means the local “tremblings” are suppressed.</p>
<table>
<thead>
<tr>
<th style="text-align:center">frequency</th>
<th style="text-align:center">1</th>
<th style="text-align:center">3</th>
<th style="text-align:center">5</th>
<th style="text-align:center">7</th>
<th style="text-align:center">9</th>
<th style="text-align:center">11</th>
<th style="text-align:center">13</th>
<th style="text-align:center">15</th>
<th style="text-align:center">17</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\( \lambda = 0 \) variance</td>
<td style="text-align:center">0.0342</td>
<td style="text-align:center">0.0774</td>
<td style="text-align:center">0.0551</td>
<td style="text-align:center">0.1171</td>
<td style="text-align:center">0.0532</td>
<td style="text-align:center">4.0241</td>
<td style="text-align:center">7.8691</td>
<td style="text-align:center">40.2198</td>
<td style="text-align:center">92.4461</td>
</tr>
<tr>
<td style="text-align:center">\( \lambda = 2 \) variance</td>
<td style="text-align:center">0.0323</td>
<td style="text-align:center">0.0610</td>
<td style="text-align:center">0.0387</td>
<td style="text-align:center">0.0289</td>
<td style="text-align:center">0.0227</td>
<td style="text-align:center">0.0187</td>
<td style="text-align:center">0.0160</td>
<td style="text-align:center">0.0139</td>
<td style="text-align:center">0.0123</td>
</tr>
<tr>
<td style="text-align:center">\( \lambda = 50 \) variance</td>
<td style="text-align:center">0.0122</td>
<td style="text-align:center">0.0096</td>
<td style="text-align:center">0.0060</td>
<td style="text-align:center">0.0043</td>
<td style="text-align:center">0.0034</td>
<td style="text-align:center">0.0028</td>
<td style="text-align:center">0.0024</td>
<td style="text-align:center">0.0021</td>
<td style="text-align:center">0.0018</td>
</tr>
</tbody>
</table>
<p>The penalizing from \( \lambda \) is reduced for a lower error. It will also be easy to imply:</p>
<p>$$<br>\begin{align} &amp; \lambda \rightarrow 0 &amp; \Rightarrow \ \ \ \ &amp; \text{w}_{ridge} \rightarrow \text{w}_{LSR} \\ &amp; \lambda \rightarrow \infty &amp; \Rightarrow \ \ \ \ &amp; \text{w}_{ridge} \rightarrow 0 \end{align}<br>$$</p>
<p>So everything should be clear, the optimization of \( E_{ridge} (\text{w}) \) is similar with the LSR (do the formula derivation yourself :D). Result should be:</p>
<p>$$<br>\hat{\text{w}} = (\text{X} \text{X}^T + \lambda \text{I})^{-1} \text{X} t<br>$$</p>
<p>Here is a <a href="https://github.com/tongsucn/MachineLearningPractice/blob/master/Regression/ridge_regression.py" target="_blank" rel="external">simple python implementation</a> and the MATLAB plot:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_train</span><span class="params">(self, freq)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Perform train on input training data.</span><br><span class="line">    Args:</span><br><span class="line">        freq: Frequency for Fourier basis function: scalar</span><br><span class="line">    Returns:</span><br><span class="line">        Result trained weight vector, numpy.ndarray: (2 * freq + 1) x 1</span><br><span class="line">    """</span></span><br><span class="line">    <span class="comment"># Mapping with basis function.</span></span><br><span class="line">    ext_pnt = self.fourier_basis(self.train_data, freq)</span><br><span class="line">    penalize = self.lam * np.eye(ext_pnt.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculating and returning w.</span></span><br><span class="line">    weight = np.dot(np.transpose(ext_pnt), ext_pnt) + penalize</span><br><span class="line">    weight = np.dot(np.linalg.inv(weight), np.transpose(ext_pnt))</span><br><span class="line">    <span class="keyword">return</span> np.dot(weight, self.train_label)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_test</span><span class="params">(self, weight, freq, test_data, test_label)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Perform test on input testing data.</span><br><span class="line">    Args:</span><br><span class="line">        weight: The trained weight, numpy.ndarray: (2 * freq + 1) x 1</span><br><span class="line">        freq: The frequency for Fourier basis function: scalar</span><br><span class="line">        test_data: Test data points, numpy.ndarray: num x 1</span><br><span class="line">        test_label: Test data labels, numpy.ndarray: num x 1</span><br><span class="line">    Returns:</span><br><span class="line">        Test error, mean squared error is utilized: scalar</span><br><span class="line">    """</span></span><br><span class="line">    <span class="comment"># Mapping with basis function.</span></span><br><span class="line">    ext_pnt = self.fourier_basis(test_data, freq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Testing and calculating error.</span></span><br><span class="line">    test_res = np.dot(ext_pnt, weight)</span><br><span class="line">    <span class="keyword">return</span> self._mse(test_res, test_label)</span><br></pre></td></tr></table></figure>
<p>As above mentioned, the result with lambda 0 shows the same result with LSR.</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]RR_1.png" width="256"></p>
<p>The result with lambda 2 looks much better, the overfitting problem is well suppressed and the generally trend is also preserved.</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]RR_2.png" width="256"></p>
<p>All the results with lambda 50 look similar. Although the overfitting problem is solved, but such suppression also causes a loss to precision.</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]RR_3.png" width="256"></p>
<h2 id="Regularization_and_Selection_of_\(_\lambda_\)">Regularization and Selection of \( \lambda \)</h2><p>Ridge Regression introduces a method to prevent overfitting. This method penalizes the size of weighting vector \( \text{w} \) with a suitable coefficient \( \lambda \). Such method, is called <a href="https://goo.gl/aR8UKA" target="_blank" rel="external">Regularization</a>. As above description, a “suitable” \( \lambda \) is the key to get a correct result. This chapter will introduce some information about regularization and the selection of \( \lambda \).</p>
<h3 id="Regularization">Regularization</h3><p><a href="https://goo.gl/aR8UKA" target="_blank" rel="external">Wikipedia</a>:</p>
<p><em>Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.</em></p>
<p>The most common “introduced information” in Machine Learning is appending a variable to the error function, which is composed of the penalizing coefficient and <a href="https://goo.gl/nKVaae" target="_blank" rel="external">L1-norm</a> or <a href="https://goo.gl/paVPK2" target="_blank" rel="external">L2-norm</a> form weighting vector. Our Ridge Regression use the L2-norm of the weighting vector.</p>
<p>Except Ridge Regression, there are also other methods utilizing the regularization. For example, <a href="http://statweb.stanford.edu/~tibs/lasso.html" target="_blank" rel="external">Lasso Regression</a>, which use L1-norm of weighting vector. (Maybe I would talk about it in the future posts)</p>
<h3 id="Selection_of_\(_\lambda_\)">Selection of \( \lambda \)</h3><p>Of course, the value of \( \lambda \) could be obtained by empirical knowledge, but this is not always the good choice (or we can just say it’s a bad idea).</p>
<p>A good solution is that trying \( \lambda \) with different values on some training sets and testing sets. But the problem is, in general, good training testing sets could be very expensive. Therefore, with one single training/test set, we can utilize the <a href="https://goo.gl/kesv1H" target="_blank" rel="external">cross-validation</a>.</p>
<p>Among different types of cross-validation, the most intuitive one is the “leave-one-out” cross-validation. At first, it equally divides the data sets into \( n \) subsets. Select one of these subsets as the test set, and the remaining subsets are the training sets. Doing the training/testing for n times (with different testing sets), then a relative more suitable \( \lambda \) value is obtained.</p>
<p>“Leave-one-out” cross-validation is a kind of <a href="https://goo.gl/wDEXjt" target="_blank" rel="external">exhaustive cross-validation</a>. There are still some non-exhaustive cross-validation like “k-fold” cross-validation.</p>
<h2 id="Summary">Summary</h2><p>Comparing with LSR, Ridge Regression with suitable penalizing on weighting vector \( \text{w} \)’s size can dramatically reduce the effect from overfitting. Such technique is called regularization. Hope they can help you :D</p>
<p>[1] <a href="https://github.com/tongsucn/MachineLearningPractice/blob/master/Regression/least_squares_regression.py" target="_blank" rel="external">Python code</a><br>[2] <a href="http://www.vision.rwth-aachen.de/teaching/advanced-machine-learning/winter-15-16/advanced-machine-learning" target="_blank" rel="external">Training &amp; Testing Data</a><br>[3] <a href="http://web.as.uky.edu/statistics/users/pbreheny/764-F11/notes/9-1.pdf" target="_blank" rel="external">Ridge Regression</a><br>[4] <a href="https://goo.gl/aR8UKA" target="_blank" rel="external">Regularization</a><br>[5] <a href="https://goo.gl/nKVaae" target="_blank" rel="external">L1-norm</a><br>[6] <a href="https://goo.gl/paVPK2" target="_blank" rel="external">L2-norm</a><br>[7] <a href="http://statweb.stanford.edu/~tibs/lasso.html" target="_blank" rel="external">Lasso Regression</a><br>[7] <a href="https://goo.gl/BKKNg5" target="_blank" rel="external">Cross-validation</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(', '\\)']]}});
</script>
<script type="text/javascript]]>
    </summary>
    
      <category term="algorithms" scheme="http://tongsucn.com/tags/algorithms/"/>
    
      <category term="machine learning" scheme="http://tongsucn.com/tags/machine-learning/"/>
    
      <category term="regression" scheme="http://tongsucn.com/tags/regression/"/>
    
      <category term="regularization" scheme="http://tongsucn.com/tags/regularization/"/>
    
      <category term="ridge regression" scheme="http://tongsucn.com/tags/ridge-regression/"/>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[ML Review] Least Squares Regression]]></title>
    <link href="http://tongsucn.com/2015/11/05/ML-Review-Least-Squares-Regression/"/>
    <id>http://tongsucn.com/2015/11/05/ML-Review-Least-Squares-Regression/</id>
    <published>2015-11-05T21:32:20.000Z</published>
    <updated>2015-11-13T21:53:54.000Z</updated>
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(', '\\)']]}});
</script>
<script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

<h2 id="Introduction">Introduction</h2><p>In machine learning, regression is utilized to predict continuous values. Least squares regression is one of the simplest regression technology. Given the training set: \( \text{X} = \{x_1, …, x_N\} \) with target values \( T = \{t_1, …, t_N\} \). Our goal is to obtain a function, which can “best” describe these points and predict other input points’ target values. But how can we obtain such a function?</p>
<h2 id="Linear">Linear</h2><p>Firstly, we need to “guess” the form of this function. The simplest is the linear function: \( y = kx + b \), just like what we have learnt in middle school. Let’s write it as \( t = w \cdot x + w_0 \), where \(t\) is the target value, \(w\) is the weight and \(w_0\) is bias. Therefore, to get this function, our goal now becomes calculating the value of \(w\) and \(w_0\).</p>
<p>It is obvious that we need to use some points to calculate the weight and bias. Such points named training sets. In general, training sets contain two parts: some data points \(\text{X} = [x_1, …, x_N] \) and their labels (or target values) \( t = [t_1, …, t_N] \). The formula becomes \( w^T \text{X} + w_0 = t \). In most cases we can write it as \( \widetilde{\text{w}}^T \widetilde{\text{X}} = t \), where \( \widetilde{\text{w}} = [w, w_0]^T \), \( \widetilde{\text{X}} = [\widetilde{x}_1, …, \widetilde{x}_N] \) and \( {\widetilde{x}}_i = [x_i, 1]^T \).</p>
<p>How to calculate \( \widetilde{\text{w}} \) now? Let’s consider about it from another perspective. If a suitable \( \widetilde{\text{w}} \) is given, we can easily calculate a set of labels \( t^\prime \) for the above training set \( \widetilde{\text{X}} \). But in general the components in \( t - t^\prime \) cannot be zero because the points don’t always (or we can say never) fit perfectly. (See the picture below, click to enlarge)</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_1.png" width="256"></p>
<p>In other words, if there exists a \( \widetilde{\text{w}} \), which makes the difference between \( t^\prime \) and \( t \) minimal, then this linear function with \( \widetilde{\text{w}} \) can “best” describe these points. Such \( \widetilde{\text{w}} \) is what we want. Therefore we can get it via computing the derivative of error function \( E(\widetilde{\text{w}}) = \left|\left| \widetilde{\text{w}}^T \widetilde{\text{X}}- t \right|\right|^2  \):</p>
<p>$$<br>\begin{align} &amp;&amp; \frac{\partial E(\widetilde{\text{w}})}{\partial \widetilde{\text{w}}} &amp; = 2 \widetilde{\text{X}} (\widetilde{\text{w}}^T \widetilde{\text{X}} - t) \stackrel{!}{=} 0 \\ &amp; \Rightarrow &amp; \widetilde{\text{X}} \widetilde{\text{X}}^T \widetilde{\text{w}} &amp; = \widetilde{\text{X}} t \\ &amp; \Rightarrow &amp; \quad \widetilde{\text{w}} &amp; = (\widetilde{\text{X}} \widetilde{\text{X}}^T)^{-1} \widetilde{\text{X}} t \end{align}<br>$$</p>
<p>The <a href="https://github.com/tongsucn/MachineLearningPractice/blob/master/Regression/least_squares_regression.py" target="_blank" rel="external">Python code</a>. (Python 3 + numpy)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">training_data_point = ...</span><br><span class="line">training_data_label = ...</span><br><span class="line">training_data_num = ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lsr_train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Training least squares regression (linear).</span><br><span class="line">    Returns:</span><br><span class="line">        The weights w (2 x 1).</span><br><span class="line">    """</span></span><br><span class="line">    <span class="comment"># Extending data points with an additional dimensional of value one</span></span><br><span class="line">    ext_points = ext_data(training_data_point, training_data_num)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculating and returning w</span></span><br><span class="line">    res_w = np.dot(ext_points, np.transpose(ext_points))</span><br><span class="line">    res_w = np.dot(np.linalg.inv(res_w), ext_points)</span><br><span class="line">    <span class="keyword">return</span> np.dot(res_w, np.transpose(training_data_label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ext_data</span><span class="params">(target_data, length, axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Extending data points with an additional dimension of value one.</span><br><span class="line">    Args:</span><br><span class="line">        target_data: The 1-D data points to be extended (num x 1).</span><br><span class="line">        length: The number of input data points (scalar).</span><br><span class="line">        axis: The dimension to be extended, 0 by default.</span><br><span class="line">    Returns:</span><br><span class="line">        The extended data points (num x 2).</span><br><span class="line">    """</span></span><br><span class="line">    one = np.ones((length, <span class="number">1</span>)) <span class="keyword">if</span> axis <span class="keyword">else</span> np.ones((<span class="number">1</span>, length))</span><br><span class="line">    <span class="keyword">return</span> np.concatenate((target_data, one), axis)</span><br></pre></td></tr></table></figure>
<p>Let’s see its result on a given training set:</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_2.png" width="256"></p>
<h2 id="Non-linear">Non-linear</h2><p>Although the obtained line describes the trend of these <a href="https://github.com/tongsucn/MachineLearningPractice/blob/master/Regression/regTrain.txt" target="_blank" rel="external">data points</a>, but it’s not what we want. Why? Because it doesn’t fit the points well, our function is linear but the data points implies a non-linear model! Our “guess” is not suitable for this case. So the solution should be clear: using some non-linear functions instead of the pure linear one, e.g. in polynomial form like \(t = ax^2 + bx + c\) (of course this function is not suitable for the above case, it’s just an example :P), or in Fourier form like \(t = \cos{(2 \pi x)} + \sin{(2 \pi x)} + 1\).</p>
<p>There could be many different forms of such non-linear functions, but for convenience, we need to unify them into a linear form: \(t = \text{w}^T \phi{(\text{X})}\). The \(\phi\) here is the so called <a href="https://en.wikipedia.org/wiki/Basis_function" target="_blank" rel="external">basis function</a>, it maps our points (1-D in our case) into a non-linear form for example \(\phi{(x)} = (1, x, x^2)^T\). The weight \(\text{w}\) now should be in form of \((w_0, w_1, w_2)^T\). In this case, our function will look like \(t = \text{w}^T \phi{(\text{X})} = w_0 + w_1 x + w_2 x^2\), where \(w_0\) is the bias (so the basis function should provide a \(1\) component for the bias in the result like ours). It’s now in non-linear form! Let’s perform it on our data with basis function:</p>
<p>$$ \phi{(x)} = (\phi_{0} (x), \phi_{1} (x), \phi_{2} (x), …) $$</p>
<p>where</p>
<p>$$<br>\begin{align} \phi_{0} (x) &amp; = 1 \\  \phi_{2n - 1} (x) &amp; = \frac{\cos{(2 \pi nx)}}{n} \\ \phi_{2n} (x) &amp; = \frac{\sin{(2 \pi nx)}}{n} \end{align}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lsr_train</span><span class="params">(freq)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Training least squares regression (non-linear).</span><br><span class="line">    Returns:</span><br><span class="line">        The weights w ((2 x freq + 1) x 1).</span><br><span class="line">    """</span></span><br><span class="line">    <span class="comment"># Extending data points with an additional dimensional of value one</span></span><br><span class="line">    ext_points = basis_function(training_data_point, training_data_num, freq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculating and returning w</span></span><br><span class="line">    res_w = np.dot(ext_points, np.transpose(ext_points))</span><br><span class="line">    res_w = np.dot(np.linalg.inv(res_w), ext_points)</span><br><span class="line">    <span class="keyword">return</span> np.dot(res_w, np.transpose(training_data_label))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">basis_func</span><span class="params">(input_data, data_num, freq)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Mapping 1-D data points using basis function</span><br><span class="line">    Args:</span><br><span class="line">        input_data: The 1-D data points (1 x data_num).</span><br><span class="line">        data_num: The number of input data points (scalar).</span><br><span class="line">        freq: The frequency, used to calculate components number (scalar).</span><br><span class="line">    Returns:</span><br><span class="line">        The mapped data points ((2 x freq + 1) x data_num).</span><br><span class="line">    """</span></span><br><span class="line">    nf_points = np.ndarray((<span class="number">2</span> * freq + <span class="number">1</span>, data_num))</span><br><span class="line">    nf_points[<span class="number">0</span>, :] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, freq + <span class="number">1</span>):</span><br><span class="line">        nf_points[<span class="number">2</span> * k - <span class="number">1</span>, :] = np.cos(<span class="number">2</span> * np.pi * k * input_data) / k</span><br><span class="line">        nf_points[<span class="number">2</span> * k, :] = np.sin(<span class="number">2</span> * np.pi * k * input_data) / k</span><br><span class="line">    <span class="keyword">return</span> nf_points</span><br></pre></td></tr></table></figure>
<p>Let’s see the results with different \( n \) values (i.e. different frequency values).</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_3.png" width="256"></p>
<p>At \( n = 1 \), the curve fits the trianing points not bad. When it increases to \( 3 \) and \( 5 \), the fitting looks much better. But after \( n = 7 \), things become out of control. The curves begin to “tremble” locally, they seem to try to fit every point in the training set. As a human, we definitely know that the correct result should look like the \( n = 3\) or \( n = 5 \) cases. That is, if we give the curve of \( n = 17 \) some test points, the curve will output worse results than the \( n = 3 \) or \( n = 5 \) cases, although it could better fit the training points. Such situation is called <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" rel="external">overfittting</a>.</p>
<p>The comparison of training and testing error rate is shown in the belowing picture. The <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="external">mean squared error</a> is utilized here. It’s obvious that training MSE decreased all the time, but testing MSE experienced decreasement firstly, then dramatically increased.</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_4.png" width="256"></p>
<h2 id="Summary">Summary</h2><p>Therefore, in least squares regression, more complex models (e.g. larger \( n \)) don’t always mean better predication results. Both selection of basis function and overfitting are challenges. I will review a solution to overfitting of least squares regression in next article. Some more general methods will also be introduced in the future. Hope they can help you :D</p>
<h2 id="Reference">Reference</h2><p>[1] <a href="https://github.com/tongsucn/MachineLearningPractice/blob/master/Regression/least_squares_regression.py" target="_blank" rel="external">Python code</a><br>[2] <a href="http://www.vision.rwth-aachen.de/teaching/advanced-machine-learning/winter-15-16/advanced-machine-learning" target="_blank" rel="external">Training &amp; Testing Data</a><br>[3] <a href="https://en.wikipedia.org/wiki/Basis_function" target="_blank" rel="external">Basis function</a><br>[4] <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" rel="external">Overfitting</a><br>[5] <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="external">Mean Squared Error</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(', '\\)']]}});
</script>
<script type="text/javascript]]>
    </summary>
    
      <category term="algorithms" scheme="http://tongsucn.com/tags/algorithms/"/>
    
      <category term="basis function" scheme="http://tongsucn.com/tags/basis-function/"/>
    
      <category term="least squares regression" scheme="http://tongsucn.com/tags/least-squares-regression/"/>
    
      <category term="machine learning" scheme="http://tongsucn.com/tags/machine-learning/"/>
    
      <category term="mean squared error" scheme="http://tongsucn.com/tags/mean-squared-error/"/>
    
      <category term="regression" scheme="http://tongsucn.com/tags/regression/"/>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[First Post]]></title>
    <link href="http://tongsucn.com/2015/03/06/First-Post/"/>
    <id>http://tongsucn.com/2015/03/06/First-Post/</id>
    <published>2015-03-06T08:30:20.000Z</published>
    <updated>2015-10-30T10:45:13.000Z</updated>
    <content type="html"><![CDATA[<p>Hello world!</p>
<p>This is my new blog. I will begin to record my life and study here. My interest are Computer Vision and Machine Learning.</p>
<p>It will be greatly honoured if my words here could help you.</p>
<p>Tong Su</p>
<p>MathJax Test:</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>$$E=mc^2$$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hello world!</p>
<p>This is my new blog. I will begin to record my life and study here. My interest are Computer Vision and Machine Learn]]>
    </summary>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
</feed>