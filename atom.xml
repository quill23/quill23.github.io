<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[TTimeline]]></title>
  <subtitle><![CDATA[星辰大海]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://tongsucn.com/"/>
  <updated>2015-11-11T14:36:04.000Z</updated>
  <id>http://tongsucn.com/</id>
  
  <author>
    <name><![CDATA[Tong Su]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[[ML Review] Least Squares Regression I]]></title>
    <link href="http://tongsucn.com/2015/11/05/ML-Review-Least-Squares-Regression-I/"/>
    <id>http://tongsucn.com/2015/11/05/ML-Review-Least-Squares-Regression-I/</id>
    <published>2015-11-05T21:32:20.000Z</published>
    <updated>2015-11-11T14:36:04.000Z</updated>
    <content type="html"><![CDATA[<script tpye"text="" x-mathjax-config"="">
MathJax.Hub.Config({tex2jax: {inlinMath: [['\\(', '\\)']]}});
</script>
<script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

<h2 id="Introduction">Introduction</h2><p>In machine learning, regression is utilized to predict continuous values. Least squares regression is one of the simplest regression technology. Given the training set: \( \text{X} = \{x_1, …, x_N\} \) with target values \( T = \{t_1, …, t_N\} \). Our goal is to obtain a function, which can “best” describe these points and predict other input points’ target values. But how can we obtain such a function?</p>
<h2 id="Linear">Linear</h2><p>Firstly, we need to “guess” the form of this function. The simplest is the linear function: \( y = kx + b \), just like what we have learnt in middle school. Let’s write it as \( t = w \cdot x + w_0 \), where \(t\) is the target value, \(w\) is the weight and \(w_0\) is bias. Therefore, to get this function, our goal now becomes calculating the value of \(w\) and \(w_0\).</p>
<p>It is obvious that we need to use some points to calculate the weight and bias. Such points named training sets. In general, training sets contain two parts: some data points \(\text{X} = [x_1, …, x_N] \) and their labels (or target values) \( t = [t_1, …, t_N] \). The formula becomes \( w^T \text{X} + w_0 = t \). In most cases we can write it as \( \widetilde{\text{w}}^T \widetilde{\text{X}} = t \), where \( \widetilde{\text{w}} = [w, w_0]^T \), \( \widetilde{\text{X}} = [\widetilde{x}_1, …, \widetilde{x}_N] \) and \( {\widetilde{x}}_i = [x_i, 1]^T \).</p>
<p>How to calculate \( \widetilde{\text{w}} \) now? Let’s consider about it from another perspective. If a suitable \( \widetilde{\text{w}} \) is given, we can easily calculate a set of labels \( t^\prime \) for the above training set \( \widetilde{\text{X}} \). But in general the components in \( t - t^\prime \) cannot be zero because the points don’t always (or we can say never) fit perfectly. (See the picture below, click to enlarge)</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_1.png" width="256"></p>
<p>In other words, if there exists a \( \widetilde{\text{w}} \), which makes the difference between \( t^\prime \) and \( t \) minimal, then this linear function with \( \widetilde{\text{w}} \) can “best” describe these points. Such \( \widetilde{\text{w}} \) is what we want. Therefore we can get it via computing the derivative of \( E(\widetilde{\text{w}}) = \left|\left| \widetilde{\text{w}}^T \widetilde{\text{X}}- t \right|\right|^2  \):</p>
<p>$$<br>\begin{align} &amp;&amp; \frac{\partial E(\widetilde{\text{w}})}{\partial \widetilde{\text{w}}} &amp; = 2 \widetilde{\text{X}} (\widetilde{\text{w}}^T \widetilde{\text{X}} - t) \stackrel{!}{=} 0 \\ &amp; \Rightarrow &amp; \widetilde{\text{X}} \widetilde{\text{X}}^T \widetilde{\text{w}} &amp; = \widetilde{\text{X}} t \\ &amp; \Rightarrow &amp; \quad \widetilde{\text{w}} &amp; = (\widetilde{\text{X}} \widetilde{\text{X}}^T)^{-1} \widetilde{\text{X}} t \end{align}<br>$$</p>
<p>The <a href="https://github.com/tongsucn/MachineLearningPractice/tree/master/Regression" target="_blank" rel="external">Python code</a>. (Python 3 + numpy)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">training_data_point = ...</span><br><span class="line">training_data_label = ...</span><br><span class="line">training_data_num = ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lsr_train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Training least squares regression (linear).</span><br><span class="line">    Returns:</span><br><span class="line">        The weights w (2 x 1).</span><br><span class="line">    """</span></span><br><span class="line">    <span class="comment"># Extending data points with an additional dimensional of value one</span></span><br><span class="line">    ext_points = ext_data(training_data_point, training_data_num)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculating and returning w</span></span><br><span class="line">    res_w = np.dot(ext_points, np.transpose(ext_points))</span><br><span class="line">    res_w = np.dot(np.linalg.inv(res_w), ext_points)</span><br><span class="line">    <span class="keyword">return</span> np.dot(res_w, np.transpose(training_data_label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ext_data</span><span class="params">(target_data, length, axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Extending data points with an additional dimension of value one.</span><br><span class="line">    Args:</span><br><span class="line">        target_data: The 1-D data points to be extended (num x 1).</span><br><span class="line">        length: The number of input data points (scalar).</span><br><span class="line">        axis: The dimension to be extended, 0 by default.</span><br><span class="line">    Returns:</span><br><span class="line">        The extended data points (num x 2).</span><br><span class="line">    """</span></span><br><span class="line">    one = np.ones((length, <span class="number">1</span>)) <span class="keyword">if</span> axis <span class="keyword">else</span> np.ones((<span class="number">1</span>, length))</span><br><span class="line">    <span class="keyword">return</span> np.concatenate((target_data, one), axis)</span><br></pre></td></tr></table></figure>
<p>Let’s see its result on a given training set:</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_2.png" width="256"></p>
<h2 id="Non-linear">Non-linear</h2><p>Although the obtained line describes the trend of these <a href="https://github.com/tongsucn/MachineLearningPractice/blob/master/Regression/regTrain.txt" target="_blank" rel="external">data points</a>, but it’s not what we want. Why? Because it doesn’t fit the points well, our function is linear but the data points implies a non-linear model! Our “guess” is not suitable for this case. So the solution should be clear: using some non-linear functions instead of the pure linear one, e.g. in polynomial form like \(t = ax^2 + bx + c\) (of course this function is not suitable for the above case, it’s just an example :P), or in Fourier form like \(t = \cos{(2 \pi x)} + \sin{(2 \pi x)} + 1\).</p>
<p>There could be many different forms of such non-linear functions, but for convenience, we need to unify them into a linear form: \(t = \text{w}^T \phi{(\text{X})}\). The \(\phi\) here is the so called <a href="https://en.wikipedia.org/wiki/Basis_function" target="_blank" rel="external">basis function</a>, it maps our points (1-D in our case) into a non-linear form for example \(\phi{(x)} = (1, x, x^2)^T\). The weight \(\text{w}\) now should be in form of \((w_0, w_1, w_2)^T\). In this case, our function will look like \(t = \text{w}^T \phi{(\text{X})} = w_0 + w_1 x + w_2 x^2\), where \(w_0\) is the bias (so the basis function should provide a \(1\) component for the bias in the result like ours). It’s now in non-linear form! Let’s perform it on our data with basis function:</p>
<p>$$ \phi{(x)} = (\phi_{0} (x), \phi_{1} (x), \phi_{2} (x), …) $$</p>
<p>where</p>
<p>$$<br>\begin{align} \phi_{0} (x) &amp; = 1 \\  \phi_{2n - 1} (x) &amp; = \frac{\cos{(2 \pi nx)}}{n} \\ \phi_{2n} (x) &amp; = \frac{\sin{(2 \pi nx)}}{n} \end{align}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lsr_train</span><span class="params">(freq)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Training least squares regression (non-linear).</span><br><span class="line">    Returns:</span><br><span class="line">        The weights w ((2 x freq + 1) x 1).</span><br><span class="line">    """</span></span><br><span class="line">    <span class="comment"># Extending data points with an additional dimensional of value one</span></span><br><span class="line">    ext_points = basis_function(training_data_point, training_data_num, freq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculating and returning w</span></span><br><span class="line">    res_w = np.dot(ext_points, np.transpose(ext_points))</span><br><span class="line">    res_w = np.dot(np.linalg.inv(res_w), ext_points)</span><br><span class="line">    <span class="keyword">return</span> np.dot(res_w, np.transpose(training_data_label))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">basis_func</span><span class="params">(input_data, data_num, freq)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Mapping 1-D data points using basis function</span><br><span class="line">    Args:</span><br><span class="line">        input_data: The 1-D data points (1 x data_num).</span><br><span class="line">        data_num: The number of input data points (scalar).</span><br><span class="line">        freq: The frequency, used to calculate components number (scalar).</span><br><span class="line">    Returns:</span><br><span class="line">        The mapped data points ((2 x freq + 1) x data_num).</span><br><span class="line">    """</span></span><br><span class="line">    nf_points = np.ndarray((<span class="number">2</span> * freq + <span class="number">1</span>, data_num))</span><br><span class="line">    nf_points[<span class="number">0</span>, :] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, freq + <span class="number">1</span>):</span><br><span class="line">        nf_points[<span class="number">2</span> * k - <span class="number">1</span>, :] = np.cos(<span class="number">2</span> * np.pi * k * input_data) / k</span><br><span class="line">        nf_points[<span class="number">2</span> * k, :] = np.sin(<span class="number">2</span> * np.pi * k * input_data) / k</span><br><span class="line">    <span class="keyword">return</span> nf_points</span><br></pre></td></tr></table></figure>
<p>Let’s see the results with different \( n \) values (i.e. different frequency values).</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_3.png" width="256"></p>
<p>At \( n = 1 \), the curve fits the trianing points not bad. When it increases to \( 3 \) and \( 5 \), the fitting looks much better. But after \( n = 7 \), things become out of control. The curves begin to “tremble” locally, they seem to try to fit every point in the training set. As a human, we definitely know that the correct result should look like the \( n = 3\) or \( n = 5 \) cases. That is, if we give the curve of \( n = 17 \) some test points, the curve will ouput worse results than the \( n = 3 \) or \( n = 5 \) cases, although it could better fit the training points. Such situation is called <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" rel="external">overfittting</a>.</p>
<p>The comparison of training and testing error rate is shown in the belowing picture. The <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="external">mean squared error</a> is utilized here. It’s obvious that training MSE decreased all the time, but testing MSE experienced decreasement firstly, then dramatically increased.</p>
<p><img src="http://7xo3hd.com1.z0.glb.clouddn.com/[ML_Review]LSR_4.png" width="256"></p>
<h2 id="Summary">Summary</h2><p>Therefore, in least squares regression, more complex models (e.g. larger\( n \)) don’t always mean better predication results. Both selection of basis function and overfitting are chanllenges. I will provide more deeper principles of least squares regression in next article. Some more general methods will also be introduced in the future. Hope they can help you :D</p>
<h2 id="Reference">Reference</h2><p>[1] <a href="https://github.com/tongsucn/MachineLearningPractice/tree/master/Regression" target="_blank" rel="external">Python code</a><br>[2] <a href="http://www.vision.rwth-aachen.de/teaching/advanced-machine-learning/winter-15-16/advanced-machine-learning" target="_blank" rel="external">Training &amp; Testing Data</a><br>[3] <a href="https://en.wikipedia.org/wiki/Basis_function" target="_blank" rel="external">Basis function</a><br>[4] <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" rel="external">Overfitting</a><br>[5] <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="external">Mean Squared Error</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<script tpye"text="" x-mathjax-config"="">
MathJax.Hub.Config({tex2jax: {inlinMath: [['\\(', '\\)']]}});
</script>
<script type="text/javasc]]>
    </summary>
    
      <category term="algorithms" scheme="http://tongsucn.com/tags/algorithms/"/>
    
      <category term="basis function" scheme="http://tongsucn.com/tags/basis-function/"/>
    
      <category term="least squares regression" scheme="http://tongsucn.com/tags/least-squares-regression/"/>
    
      <category term="machine learning" scheme="http://tongsucn.com/tags/machine-learning/"/>
    
      <category term="mean squared error" scheme="http://tongsucn.com/tags/mean-squared-error/"/>
    
      <category term="regression" scheme="http://tongsucn.com/tags/regression/"/>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[First Post]]></title>
    <link href="http://tongsucn.com/2015/03/06/First-Post/"/>
    <id>http://tongsucn.com/2015/03/06/First-Post/</id>
    <published>2015-03-06T08:30:20.000Z</published>
    <updated>2015-10-30T10:45:13.000Z</updated>
    <content type="html"><![CDATA[<p>Hello world!</p>
<p>This is my new blog. I will begin to record my life and study here. My interest are Computer Vision and Machine Learning.</p>
<p>It will be greatly honoured if my words here could help you.</p>
<p>Tong Su</p>
<p>MathJax Test:</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>$$E=mc^2$$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hello world!</p>
<p>This is my new blog. I will begin to record my life and study here. My interest are Computer Vision and Machine Learn]]>
    </summary>
    
      <category term="blog" scheme="http://tongsucn.com/categories/blog/"/>
    
  </entry>
  
</feed>